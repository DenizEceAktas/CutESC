{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from stat import ST_DEV\n",
    "from sys import argv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import scipy.spatial\n",
    "import scipy.sparse\n",
    "import networkx as netx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear = lambda: os.system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CutESC algorithm starts here and in the code below a spatial graph is generated using Delaunay graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CutESC(data):\n",
    "    tri = sp.spatial.Delaunay(data)                   # delaunay generator\n",
    "    lil = sp.sparse.lil_matrix((tri.points, tri.points))     #turn the dealunay triangulars to matrix\n",
    "    indices, indptr = tri.vertex_neighbor_vertices\n",
    "    for k in range(tri.points):\n",
    "        lil.rows[k] = indptr[indices[k]:indices[k + 1]]\n",
    "        lil.data[k] = np.ones_like(lil.rows[k])               #example data of same shape as row that is filled with ones\n",
    "    coo = lil.tocoo()                             #coo format to access to each node edge node pair\n",
    "    conns = np.vstack((coo.row, coo.col)).T           #connections have two nodes at the ends of the edge\n",
    "    delaunay_conns = np.sort(conns, axis=1)              # this is the edges(delaunay connnections)\n",
    "    graph = netx.Graph(delaunay_conns)\n",
    "    nodes = dict(graph.nodes())\n",
    "    for i in nodes:\n",
    "        nodes[i]['node'] = np.array(data[i])             #add nodes\n",
    "    adjacency = dict(graph.adjacency())\n",
    "    removed = []\n",
    "    for i in nodes:                                      #add weights to nodes\n",
    "        for j in adjacency[i]:\n",
    "            weight = np.linalg.norm(nodes[i]['node'] - nodes[j]['node'])\n",
    "            if weight != 0:\n",
    "                graph.edges[(i, j)]['weight'] = weight\n",
    "            else:\n",
    "                removed.append((i, j))\n",
    "    graph.remove_edges_from(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this part of the code gabriel graph is formed from delaunay triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tree = scipy.spatial.cKDTree(data)\n",
    "    c = tri.points[delaunay_conns]            # center of each edge\n",
    "    m = (c[:, 0, :] + c[:, 1, :]) / 2            # midpoint of each edge\n",
    "    r = np.sqrt(np.sum((c[:, 0, :] - c[:, 1, :]) ** 2, axis=1)) / 2            # radius\n",
    "    n = tree.query(x=m, k=1)[0]                 #the closest point for each midpoint\n",
    "    if n >= r * 0.999:\n",
    "        g = n                         #closest point to midpoint is at a distance r, then it is a Gabriel edge\n",
    "    gabriel_conns = delaunay_conns[g]  # gabriel edges\n",
    "\n",
    "    graph2 = netx.Graph(gabriel_conns)\n",
    "    nodes2 = dict(graph2.nodes())\n",
    "    for i in nodes2:\n",
    "        nodes2[i]['node'] = np.array(data[i])       #add nodes\n",
    "    adjacency2 = dict(graph2.adjacency())\n",
    "    removed2 = []\n",
    "    for i in nodes2:                                  #add weights to nodes\n",
    "        for j in adjacency2[i]:\n",
    "            weight2 = np.linalg.norm(nodes2[i]['node'] - nodes2[j]['node'])\n",
    "            if weight2 != 0:\n",
    "                graph2.edges[(i, j)]['weight'] = weight2\n",
    "            else:\n",
    "                removed2.append((i, j))\n",
    "    graph2.remove_edges_from(removed2)\n",
    "    nodes2 = dict(graph2.nodes())\n",
    "    adjacency2 = dict(graph2.adjacency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this part of the graph globally long edges are found and removed from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    node_num = graph2.number_of_nodes()\n",
    "    edges = dict(graph2.edges())\n",
    "    weights = list(netx.get_edge_attributes(graph2, 'weight').values())\n",
    "    mean_loc = list()\n",
    "    for i in nodes2:                          #the mean length of incident edges of vertex Pi\n",
    "        loc_m = 0\n",
    "        for j in adjacency2[i]:\n",
    "            loc_m = loc_m + graph2.edges[i, j][\"weight\"]\n",
    "        if len(adjacency2[i]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            loc_m = loc_m / len(adjacency2[i])\n",
    "        mean_loc.insert(i, loc_m)              # mean(Pi)\n",
    "    global_m = np.mean(weights)          # mean(GG)          #the mean length of all edges in gabriel graph\n",
    "    glo_std = 0\n",
    "    for i in nodes2:                            #the standard derivation of the mean length of edges in neigborhood\n",
    "        glo_std = glo_std + math.pow(global_m - mean_loc[i], 2)\n",
    "    glo_std = math.sqrt(glo_std / (node_num - 1))            # std(GG)\n",
    "    GCuti = list()                           #cut edge value\n",
    "    for i in range(node_num):\n",
    "        if mean_loc[i] == 0:\n",
    "            GCuti.append(0)\n",
    "        else:\n",
    "            var = global_m * glo_std / mean_loc[i]\n",
    "            var = var + global_m\n",
    "            GCuti.append(var)\n",
    "    remove = list()                             #remove globally long edges\n",
    "    for i in edges:\n",
    "        if (edges[i][\"weight\"] >= GCuti[i[0]]) or (edges[i][\"weight\"]) >= GCuti[i[1]]:\n",
    "            remove.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    graph2.remove_edges_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this part locally long edges found and removed from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nodes = dict(graph2.nodes())\n",
    "    adjacency = dict(graph2.adjacency())\n",
    "    node_num = graph2.number_of_nodes()\n",
    "    edges = dict(graph2.edges())\n",
    "    mean_loc = list()\n",
    "    label = list()\n",
    "    graph_var = list()\n",
    "    for var in sorted(netx.connected_components(graph2), reverse=True):\n",
    "        graph_var[var] = [var]\n",
    "    for i in range(len(graph_var)):\n",
    "        for j in graph_var[i]:\n",
    "            label.insert(j, i)\n",
    "    graph_var_m = list()\n",
    "    nei_m = 0\n",
    "    for i in nodes:  # the mean length of edges in the second order neighborhood of a vertex Pi in a subgraph Gx\n",
    "        loc_m = 0\n",
    "        nei_num = 0\n",
    "        for j in adjacency[i]:\n",
    "            weight = graph2.edges[i, j]['weight']\n",
    "            loc_m = loc_m + weight\n",
    "            nei_num = nei_num + 1\n",
    "            for n in adjacency[j]:\n",
    "                weight = graph2.edges[j, n]['weight']\n",
    "                loc_m = loc_m + weight\n",
    "                nei_num = nei_num + 1\n",
    "        if len(adjacency[i]) == 0:\n",
    "            continue\n",
    "        if nei_num == 0:\n",
    "            continue\n",
    "        if nei_num != 0:\n",
    "            nei_m = loc_m / nei_num\n",
    "        graph_var_m.insert(label[i], nei_m + graph_var_m[label[i]])  # mean(Pi)\n",
    "\n",
    "    for i in range(len(graph_var_m)):\n",
    "        graph_var_m[i] = graph_var_m[i] / len(graph_var)  # mean(Gk)\n",
    "\n",
    "    for i in nodes:  # the standard derivation of all edges that are directly connected to vertex Pi\n",
    "        loc_std = 0\n",
    "        nei_num = 0\n",
    "        for j in adjacency[i]:\n",
    "            weight = graph2.edges[i, j]['weight']\n",
    "            loc_std = loc_std + math.pow(graph_var_m[label[i]] - weight, 2)\n",
    "            nei_num = nei_num + 1\n",
    "            for n in adjacency[j]:\n",
    "                weight = graph2.edges[j, n]['weight']\n",
    "                loc_std = loc_std + math.pow(graph_var_m[label[i]] - weight, 2)\n",
    "                nei_num = nei_num + 1\n",
    "        if len(adjacency[i]) == 0:\n",
    "            continue\n",
    "        if nei_num == 0:\n",
    "            continue\n",
    "        if nei_num != 0:\n",
    "            loc_std = math.sqrt(loc_std / (nei_num - 1))\n",
    "        loc_std[i] = loc_std  # std(Pi)\n",
    "\n",
    "    alpha = 1  # should be between 0 and 1 set to 1 by default\n",
    "    remove = list()  # remove part\n",
    "    for i in edges:\n",
    "        val1 = math.exp(graph_var_m[label[i[0]]] / graph.edges[i]['weight'])\n",
    "        val2 = math.exp(graph_var_m[label[i[1]]] / graph.edges[i]['weight'])\n",
    "        val1 = alpha * loc_st[i[0]] * val1\n",
    "        val2 = alpha * loc_st[i[1]] * val2\n",
    "        val1 = graph_var_m[label[i[0]]] + val1\n",
    "        val2 = graph_var_m[label[i[1]]] + val2\n",
    "        if (edges[i]['weight'] >= val1) or (edges[i]['weight'] >= val2):\n",
    "            remove.append(i)\n",
    "    graph2.remove_edges_from(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section other locally long edges are identified and removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nodes = dict(graph2.nodes())\n",
    "    adjacency = dict(graph2.adjacency())\n",
    "    node_num = graph2.number_of_nodes()\n",
    "    edges = dict(graph2.edges())\n",
    "    loc_me = list()\n",
    "    loc_st = list()\n",
    "    for i in nodes:  # the mean length of edges in the second order neighborhood of a vertex Pi in a  new subgraph\n",
    "        loc_m = 0\n",
    "        nei_num = 0\n",
    "        for j in adjacency[i]:\n",
    "            weight = graph2.edges[i, j]['weight']\n",
    "            loc_m = loc_m + weight\n",
    "            nei_num = nei_num + 1\n",
    "            for n in adjacency[j]:\n",
    "                weight = graph2.edges[j, n]['weight']\n",
    "                loc_m = loc_m + weight\n",
    "                nei_num = nei_num + 1\n",
    "        if len(adjacency[i]) == 0:\n",
    "            continue\n",
    "        if nei_num == 0:\n",
    "            continue\n",
    "        if nei_num != 0:\n",
    "            nei_m = loc_m / nei_num\n",
    "        loc_me.insert(i, nei_m)  # mean(Pi)\n",
    "\n",
    "    for i in nodes:  # the standard derivation of all edges that are directly connected to vertex Pi\n",
    "        loc_std = 0\n",
    "        nei_num = 0\n",
    "        for j in adjacency[i]:\n",
    "            weight = graph2.edges[i, j]['weight']\n",
    "            loc_std = loc_std + math.pow(loc_me[i] - weight, 2)\n",
    "            nei_num = nei_num + 1\n",
    "            for n in adjacency[j]:\n",
    "                weight = graph2.edges[j, n]['weight']\n",
    "                loc_std = loc_std + math.pow(loc_me[i] - weight, 2)\n",
    "                nei_num = nei_num + 1\n",
    "        if len(adjacency[i]) == 0:\n",
    "            continue\n",
    "        if nei_num == 0:\n",
    "            continue\n",
    "        if nei_num != 0:\n",
    "            loc_std = math.sqrt(loc_std / (nei_num - 1))\n",
    "        loc_st.insert(i, loc_std)  # std(Pi)\n",
    "\n",
    "    beta = 1   #should be between 0 and 1 set to 1 by default\n",
    "    remove = list()  # remove part\n",
    "    for i in edges:\n",
    "        val1 = math.exp(loc_me[i[0]] / graph.edges[i]['weight'])\n",
    "        val2 = math.exp(loc_me[i[1]] / graph.edges[i]['weight'])\n",
    "        val1 = beta * loc_st[i[0]] * val1\n",
    "        val2 = beta * loc_st[i[1]] * val2\n",
    "        val1 = loc_me[i[0]] + val1\n",
    "        val2 = loc_me[i[1]] + val2\n",
    "        if (edges[i]['weight'] >= val1) or (edges[i]['weight'] >= val2):\n",
    "            remove.append(i)\n",
    "    graph2.remove_edges_from(removed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
